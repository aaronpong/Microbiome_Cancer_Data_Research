direct_lasso <- glmnet(
x = X_train,
y = y_train,
family = "binomial",
alpha = 1,
lambda = lambda_seq
)
# Choose lambda that gives ~20 features
n_features <- sapply(lambda_seq, function(l) {
sum(coef(direct_lasso, s = l)[-1] != 0)  # Count non-zero coefficients, excluding intercept
})
# Find lambda yielding closest to 20 features
target_features <- 20
best_lambda_idx <- which.min(abs(n_features - target_features))
best_lambda <- lambda_seq[best_lambda_idx]
cat("Selected lambda:", best_lambda, "giving", n_features[best_lambda_idx], "features\n")
# Extract coefficients
coef_matrix <- coef(direct_lasso, s = best_lambda)
nonzero_idx <- which(coef_matrix[-1] != 0)  # Excluding intercept
nonzero_features <- colnames(X_train)[nonzero_idx]
nonzero_coefs <- coef_matrix[nonzero_idx + 1]  # +1 for intercept offset
# Make predictions
test_probs <- predict(direct_lasso, newx = X_test, type = "response", s = best_lambda)
test_preds <- factor(ifelse(test_probs > 0.5, "Tumor", "Healthy"), levels = levels(y_test))
# Calculate confusion matrix
conf_matrix <- confusionMatrix(test_preds, y_test)
print(conf_matrix)
# Calculate ROC and AUC
roc_obj <- roc(as.numeric(y_test == "Tumor"), as.vector(test_probs))
auc_value <- auc(roc_obj)
cat("AUC:", round(auc_value, 4), "\n")
# Create feature importance dataframe
feature_importance <- data.frame(
Feature = nonzero_features,
Coefficient = nonzero_coefs
)
# Sort by absolute coefficient value
feature_importance <- feature_importance[order(-abs(feature_importance$Coefficient)), ]
# Display top 20 most important features
head(feature_importance, 20)
# Save results
raw_counts_results <- list(
model = direct_lasso,
lambda_min = best_lambda,
selected_features = feature_importance,
performance = list(
accuracy = conf_matrix$overall["Accuracy"],
sensitivity = conf_matrix$byClass["Sensitivity"],
specificity = conf_matrix$byClass["Specificity"],
balanced_accuracy = conf_matrix$byClass["Balanced Accuracy"],
auc = auc_value,
confusion_matrix = conf_matrix
)
)
} else {
# Set up train control with stratified cross-validation
train_control <- trainControl(
method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "final",
sampling = "up"  # Use upsampling to balance classes
)
# Set up glmnet tuning grid
alpha_grid <- 1  # Use LASSO
lambda_grid <- exp(seq(-8, 1, length.out = 50))
tuning_grid <- expand.grid(alpha = alpha_grid, lambda = lambda_grid)
# Train the model
set.seed(123)
lasso_model <- train(
x = X_train,
y = y_train,
method = "glmnet",
trControl = train_control,
tuneGrid = tuning_grid,
metric = "ROC",
preProcess = c("center", "scale")
)
# Print model results
print(lasso_model)
plot(lasso_model)
# Extract best lambda and corresponding model
best_lambda <- lasso_model$bestTune$lambda
cat("Best lambda:", best_lambda, "\n")
# Get final model
final_model <- lasso_model$finalModel
# Extract non-zero coefficients from the best model
coef_matrix <- coef(final_model, s = best_lambda)
nonzero_idx <- which(coef_matrix[-1, 1] != 0)  # Excluding intercept
nonzero_features <- colnames(X_train)[nonzero_idx]
nonzero_coefs <- coef_matrix[nonzero_idx + 1, 1]  # +1 for intercept offset
# Print number of selected features
cat("Number of features selected:", length(nonzero_idx), "out of", ncol(X_train), "\n")
# Create a data frame of important features and their coefficients
feature_importance <- data.frame(
Feature = nonzero_features,
Coefficient = nonzero_coefs
)
# Sort by absolute coefficient value
feature_importance <- feature_importance[order(-abs(feature_importance$Coefficient)), ]
# Display top 20 most important features
head(feature_importance, 20)
# Make predictions on the test set
test_probs <- predict(lasso_model, newdata = X_test, type = "prob")[, "Tumor"]
test_preds <- predict(lasso_model, newdata = X_test, type = "raw")
# Calculate confusion matrix
conf_matrix <- confusionMatrix(test_preds, y_test)
print(conf_matrix)
# Calculate ROC and AUC
roc_obj <- roc(as.numeric(y_test == "Tumor"), test_probs)
auc_value <- auc(roc_obj)
cat("AUC:", round(auc_value, 4), "\n")
# Save the model and results
raw_counts_results <- list(
model = lasso_model,
lambda_min = best_lambda,
selected_features = feature_importance,
performance = list(
accuracy = conf_matrix$overall["Accuracy"],
sensitivity = conf_matrix$byClass["Sensitivity"],
specificity = conf_matrix$byClass["Specificity"],
balanced_accuracy = conf_matrix$byClass["Balanced Accuracy"],
auc = auc_value,
confusion_matrix = conf_matrix
)
)
}
# Plot ROC curve
plot(roc_obj, main = "ROC Curve for Raw Counts + LASSO Model",
col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "gray")
abline(a = 0, b = 1, lty = 2, col = "gray")
legend("bottomright", legend = paste("AUC =", round(auc_value, 4)),
col = "blue", lwd = 2)
library(compositions)  # For ALR transformation
library(caret)
library(glmnet)
library(pROC)
# First, convert raw counts to compositional data
# Add a small pseudocount to avoid zeros
pseudocount <- 0.5
comp_data <- count_data + pseudocount
# Normalize to get compositional data (each row sums to 1)
comp_data <- t(apply(comp_data, 1, function(x) x/sum(x)))
# Apply ALR transformation (using the last taxon as reference by default)
alr_comp_data <- alr(comp_data)
# Apply ALR transformation (using the last taxon as reference by default)
alr_comp_data <- alr(comp_data)
# Create dataframe with ALR transformed data
alr_data_all <- data.frame(
SampleID = SampleID,
y = true_y,
subject_id = subject_id
)
# Add ALR transformed data columns
for (i in 1:ncol(alr_comp_data)) {
alr_data_all[[paste0("ALR_", i)]] <- alr_comp_data[, i]
}
# Clean up missing values if any
alr_data_clean <- alr_data_all[complete.cases(alr_data_all),]
# Verify class distribution
print("ALR data class distribution:")
print(table(alr_data_clean$y))
# Save the cleaned ALR data
write.csv(alr_data_clean, "data/alr_data_clean.csv")
# Save the cleaned ALR data
write.csv(alr_data_clean, "data/alr_data_clean.csv")
# Split into training and testing sets (use same subjects as in the raw count analysis)
X_alr <- as.matrix(alr_data_clean[, !colnames(alr_data_clean) %in% c("SampleID", "y", "subject_id")])
# Split into training and testing sets (use same subjects as in the raw count analysis)
X_alr <- as.matrix(alr_data_clean[, !colnames(alr_data_clean) %in% c("SampleID", "y", "subject_id")])
y_alr <- factor(alr_data_clean$y, levels = c("Healthy", "Tumor"))
# Use the same training and test sets as with raw counts for fair comparison
train_idx_alr <- which(alr_data_clean$subject_id %in% train_subjects)
test_idx_alr <- setdiff(1:nrow(alr_data_clean), train_idx_alr)
X_train_alr <- X_alr[train_idx_alr, ]
y_train_alr <- y_alr[train_idx_alr]
X_test_alr <- X_alr[test_idx_alr, ]
y_test_alr <- y_alr[test_idx_alr]
# Verify training and testing class distribution
print("ALR training set class distribution:")
print(table(y_train_alr))
print("ALR testing set class distribution:")
print(table(y_test_alr))
# Set up train control with stratified cross-validation
train_control_alr <- trainControl(
method = "cv",
number = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary,
savePredictions = "final",
sampling = "up"  # Use upsampling to balance classes
)
# Set up glmnet tuning grid
alpha_grid_alr <- 1  # Use LASSO
lambda_grid_alr <- exp(seq(-8, 1, length.out = 50))
tuning_grid_alr <- expand.grid(alpha = alpha_grid_alr, lambda = lambda_grid_alr)
# Train the model
set.seed(123)
alr_lasso_model <- train(
x = X_train_alr,
y = y_train_alr,
method = "glmnet",
trControl = train_control_alr,
tuneGrid = tuning_grid_alr,
metric = "ROC",
preProcess = c("center", "scale")
)
alr_lasso_model <- train(
x = X_train_alr,
y = y_train_alr,
method = "glmnet",
trControl = train_control_alr,
tuneGrid = tuning_grid_alr,
metric = "ROC",
preProcess = c("center", "scale")
)
# Print model results
print(alr_lasso_model)
plot(alr_lasso_model)
# Compare performance of Raw Counts vs ALR
results_comparison <- data.frame(
Method = c("Raw Counts + LASSO", "ALR + LASSO"),
Accuracy = c(as.numeric(raw_counts_results$performance$accuracy),
as.numeric(alr_results$performance$accuracy)),
Sensitivity = c(as.numeric(raw_counts_results$performance$sensitivity),
as.numeric(alr_results$performance$sensitivity)),
Specificity = c(as.numeric(raw_counts_results$performance$specificity),
as.numeric(alr_results$performance$specificity)),
AUC = c(as.numeric(raw_counts_results$performance$auc),
as.numeric(alr_results$performance$auc)),
Features_Selected = c(nrow(raw_counts_results$selected_features),
nrow(alr_results$selected_features))
)
# Extract best lambda and corresponding model
best_lambda_alr <- alr_lasso_model$bestTune$lambda
cat("Best lambda:", best_lambda_alr, "\n")
# Get final model
final_model_alr <- alr_lasso_model$finalModel
# Extract non-zero coefficients from the best model
coef_matrix_alr <- coef(final_model_alr, s = best_lambda_alr)
nonzero_idx_alr <- which(coef_matrix_alr[-1, 1] != 0)  # Excluding intercept
nonzero_features_alr <- colnames(X_train_alr)[nonzero_idx_alr]
nonzero_coefs_alr <- coef_matrix_alr[nonzero_idx_alr + 1, 1]  # +1 for intercept offset
# Print number of selected features
cat("Number of features selected:", length(nonzero_idx_alr), "out of", ncol(X_train_alr), "\n")
# Create a data frame of important features and their coefficients
feature_importance_alr <- data.frame(
Feature = nonzero_features_alr,
Coefficient = nonzero_coefs_alr
)
# Sort by absolute coefficient value
feature_importance_alr <- feature_importance_alr[order(-abs(feature_importance_alr$Coefficient)), ]
# Display top 20 most important features
print("Top 20 most important ALR features:")
print(head(feature_importance_alr, 20))
# Make predictions on the test set
test_probs_alr <- predict(alr_lasso_model, newdata = X_test_alr, type = "prob")[, "Tumor"]
test_preds_alr <- predict(alr_lasso_model, newdata = X_test_alr, type = "raw")
# Calculate confusion matrix
conf_matrix_alr <- confusionMatrix(test_preds_alr, y_test_alr)
print(conf_matrix_alr)
# Calculate ROC and AUC
roc_obj_alr <- roc(as.numeric(y_test_alr == "Tumor"), test_probs_alr)
auc_value_alr <- auc(roc_obj_alr)
cat("AUC:", round(auc_value_alr, 4), "\n")
# Plot ROC curve
plot(roc_obj_alr, main = "ROC Curve for ALR + LASSO Model",
col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "gray")
legend("bottomright", legend = paste("AUC =", round(auc_value_alr, 4)),
col = "blue", lwd = 2)
# Save the model and results
alr_results <- list(
model = alr_lasso_model,
lambda_min = best_lambda_alr,
selected_features = feature_importance_alr,
performance = list(
accuracy = conf_matrix_alr$overall["Accuracy"],
sensitivity = conf_matrix_alr$byClass["Sensitivity"],
specificity = conf_matrix_alr$byClass["Specificity"],
balanced_accuracy = conf_matrix_alr$byClass["Balanced Accuracy"],
auc = auc_value_alr,
confusion_matrix = conf_matrix_alr
)
)
# Compare performance of Raw Counts vs ALR
results_comparison <- data.frame(
Method = c("Raw Counts + LASSO", "ALR + LASSO"),
Accuracy = c(as.numeric(raw_counts_results$performance$accuracy),
as.numeric(alr_results$performance$accuracy)),
Sensitivity = c(as.numeric(raw_counts_results$performance$sensitivity),
as.numeric(alr_results$performance$sensitivity)),
Specificity = c(as.numeric(raw_counts_results$performance$specificity),
as.numeric(alr_results$performance$specificity)),
AUC = c(as.numeric(raw_counts_results$performance$auc),
as.numeric(alr_results$performance$auc)),
Features_Selected = c(nrow(raw_counts_results$selected_features),
nrow(alr_results$selected_features))
)
print("Performance comparison between methods:")
print(results_comparison)
# Compare performance metrics
performance_comparison <- data.frame(
Method = c("Raw Counts + LASSO", "ALR Transformation + LASSO"),
Accuracy = c(raw_counts_results$performance$accuracy, alr_results$performance$accuracy),
Sensitivity = c(raw_counts_results$performance$sensitivity, alr_results$performance$sensitivity),
Specificity = c(raw_counts_results$performance$specificity, alr_results$performance$specificity),
Balanced_Accuracy = c(raw_counts_results$performance$balanced_accuracy, alr_results$performance$balanced_accuracy),
AUC = c(raw_counts_results$performance$auc, alr_results$performance$auc)
)
# Display performance comparison
print(performance_comparison)
# Visualize performance metrics comparison
performance_long <- melt(performance_comparison, id.vars = "Method")
ggplot(performance_long, aes(x = variable, y = value, fill = Method)) +
geom_bar(stat = "identity", position = "dodge", width = 0.7) +
scale_fill_manual(values = c("steelblue", "firebrick")) +
labs(title = "Performance Comparison: Raw Counts vs. ALR Transformation",
x = "Metric", y = "Value") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
geom_text(aes(label = sprintf("%.3f", value)),
position = position_dodge(width = 0.7),
vjust = -0.3, size = 3)
# Compare number of selected features
feature_count_comparison <- data.frame(
Method = c("Raw Counts + LASSO", "ALR Transformation + LASSO"),
Number_of_Features = c(nrow(raw_counts_results$selected_features),
nrow(alr_results$selected_features))
)
# Display feature count comparison
print(feature_count_comparison)
# Visualize feature count comparison
ggplot(feature_count_comparison, aes(x = Method, y = Number_of_Features, fill = Method)) +
geom_bar(stat = "identity", width = 0.5) +
scale_fill_manual(values = c("steelblue", "firebrick")) +
labs(title = "Number of Selected Features by Method",
x = "", y = "Number of Features") +
theme_minimal() +
geom_text(aes(label = Number_of_Features), vjust = -0.3)
roc_alr <- roc(y_test, as.vector(predict(alr_results$model,
newx = as.matrix(X_alr_test),
type = "response",
s = alr_results$lambda_min)))
plot(roc_raw, col = "blue", lwd = 2,
main = "ROC Curve Comparison")
lines(roc_alr, col = "red", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "gray")
legend("bottomright",
legend = c(
paste("Raw Counts (AUC =", round(raw_counts_results$performance$auc, 3), ")"),
paste("ALR (AUC =", round(alr_results$performance$auc, 3), ")")
),
col = c("blue", "red"),
lwd = 2)
# Look at overlap in selected features
if (length(raw_counts_results$selected_features$Feature) > 0 &&
length(alr_results$selected_features$Feature) > 0) {
# For ALR, we need to extract the numerator of each log-ratio
alr_numerators <- sapply(strsplit(as.character(alr_results$selected_features$Feature), "/"),
function(x) x[1])
# Find common features
common_features <- intersect(raw_counts_results$selected_features$Feature,
alr_numerators)
cat("Number of common features selected by both methods:",
length(common_features), "\n")
if (length(common_features) > 0) {
cat("Common features:\n")
print(common_features)
# Create Venn diagram-like display of feature overlap
library(VennDiagram)
venn.plot <- venn.diagram(
x = list(
"Raw Counts" = raw_counts_results$selected_features$Feature,
"ALR Transform" = alr_numerators
),
filename = NULL,
fill = c("steelblue", "firebrick"),
alpha = 0.5,
main = "Overlap in Selected Features"
)
grid.draw(venn.plot)
}
}
# Provide a summary of findings
cat("\n==== Summary of Findings ====\n")
cat("\n1. Classification Performance:\n")
if (raw_counts_results$performance$auc > alr_results$performance$auc) {
cat("   - The Raw Counts + LASSO approach achieved better overall performance with higher AUC.\n")
} else if (raw_counts_results$performance$auc < alr_results$performance$auc) {
cat("   - The ALR Transformation + LASSO approach achieved better overall performance with higher AUC.\n")
} else {
cat("   - Both approaches achieved similar overall performance in terms of AUC.\n")
}
# Comparison of classification methods for microbiome data
library(reshape2)
library(ggplot2)
library(grid)
# Compare performance metrics
performance_comparison <- data.frame(
Method = c("Raw Counts + LASSO", "ALR Transformation + LASSO"),
Accuracy = c(raw_counts_results$performance$accuracy, alr_results$performance$accuracy),
Sensitivity = c(raw_counts_results$performance$sensitivity, alr_results$performance$sensitivity),
Specificity = c(raw_counts_results$performance$specificity, alr_results$performance$specificity),
Balanced_Accuracy = c(raw_counts_results$performance$balanced_accuracy, alr_results$performance$balanced_accuracy),
AUC = c(raw_counts_results$performance$auc, alr_results$performance$auc)
)
# Display performance comparison
print(performance_comparison)
# Visualize performance metrics comparison
performance_long <- melt(performance_comparison, id.vars = "Method")
ggplot(performance_long, aes(x = variable, y = value, fill = Method)) +
geom_bar(stat = "identity", position = "dodge", width = 0.7) +
scale_fill_manual(values = c("steelblue", "firebrick")) +
labs(title = "Performance Comparison: Raw Counts vs. ALR Transformation",
x = "Metric", y = "Value") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
geom_text(aes(label = sprintf("%.3f", value)),
position = position_dodge(width = 0.7),
vjust = -0.3, size = 3)
# Compare number of selected features
feature_count_comparison <- data.frame(
Method = c("Raw Counts + LASSO", "ALR Transformation + LASSO"),
Number_of_Features = c(nrow(raw_counts_results$selected_features),
nrow(alr_results$selected_features))
)
# Display feature count comparison
print(feature_count_comparison)
# Visualize feature count comparison
ggplot(feature_count_comparison, aes(x = Method, y = Number_of_Features, fill = Method)) +
geom_bar(stat = "identity", width = 0.5) +
scale_fill_manual(values = c("steelblue", "firebrick")) +
labs(title = "Number of Selected Features by Method",
x = "", y = "Number of Features") +
theme_minimal() +
geom_text(aes(label = Number_of_Features), vjust = -0.3)
# Compare ROC curves
# This part is problematic - using the proper caret predict function instead
roc_raw <- roc(y_test,
predict(raw_counts_results$model, X_test, type = "prob")[, "Tumor"])
roc_alr <- roc(y_test_alr,
predict(alr_results$model, X_test_alr, type = "prob")[, "Tumor"])
plot(roc_raw, col = "blue", lwd = 2,
main = "ROC Curve Comparison")
lines(roc_alr, col = "red", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "gray")
legend("bottomright",
legend = c(
paste("Raw Counts (AUC =", round(raw_counts_results$performance$auc, 3), ")"),
paste("ALR (AUC =", round(alr_results$performance$auc, 3), ")")
),
col = c("blue", "red"),
lwd = 2)
# Look at overlap in selected features
if (length(raw_counts_results$selected_features$Feature) > 0 &&
length(alr_results$selected_features$Feature) > 0) {
# For ALR, we may need to modify how we extract features depending on your naming convention
# If your ALR features are named like "ALR_123", extract the number part for comparison
alr_features <- gsub("ALR_", "", as.character(alr_results$selected_features$Feature))
raw_features <- as.character(raw_counts_results$selected_features$Feature)
# Find common features
common_features <- intersect(raw_features, alr_features)
cat("Number of common features selected by both methods:",
length(common_features), "\n")
if (length(common_features) > 0) {
cat("Common features:\n")
print(common_features)
# Create Venn diagram-like display of feature overlap if VennDiagram package is available
if (requireNamespace("VennDiagram", quietly = TRUE)) {
venn.plot <- VennDiagram::venn.diagram(
x = list(
"Raw Counts" = raw_features,
"ALR Transform" = alr_features
),
filename = NULL,
fill = c("steelblue", "firebrick"),
alpha = 0.5,
main = "Overlap in Selected Features"
)
grid.newpage()
grid.draw(venn.plot)
} else {
cat("VennDiagram package not available. Install with install.packages('VennDiagram')\n")
}
}
}
# Provide a summary of findings
cat("\n==== Summary of Findings ====\n")
cat("\n1. Classification Performance:\n")
if (raw_counts_results$performance$auc > alr_results$performance$auc) {
cat("   - The Raw Counts + LASSO approach achieved better overall performance with higher AUC.\n")
} else if (raw_counts_results$performance$auc < alr_results$performance$auc) {
cat("   - The ALR Transformation + LASSO approach achieved better overall performance with higher AUC.\n")
} else {
cat("   - Both approaches achieved similar overall performance in terms of AUC.\n")
}
cat("\n2. Feature Selection:\n")
cat("   - Raw Counts + LASSO selected", nrow(raw_counts_results$selected_features), "features.\n")
cat("   - ALR Transformation + LASSO selected", nrow(alr_results$selected_features), "features.\n")
cat("\n3. Biological Insights:\n")
cat("   - Both methods identified taxonomic patterns associated with colorectal cancer.\n")
cat("   - The ALR method may provide more statistically valid insights due to accounting for the compositional nature of microbiome data.\n")
cat("   - The Raw Counts method may be more directly interpretable in terms of abundance differences between tumor and normal samples.\n")
# Recommendations for practice
cat("\n4. Recommendations:\n")
cat("   - Consider using both methods complementarily for microbiome analysis.\n")
cat("   - Raw counts may be more intuitive for initial exploratory analysis.\n")
cat("   - ALR transformation provides a statistically principled approach for compositional data.\n")
cat("   - Features consistently identified by both methods may be the most reliable biomarkers.\n")

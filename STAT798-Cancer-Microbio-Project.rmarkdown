---
title: "cancerdata"
format: html
editor: visual
---

```{r setup, include = FALSE, warning=FALSE}
#Clear Environment
rm(list = ls())
```

## Data Setup and Cleaning

Load Packages

```{r, warning=FALSE}
library(compositions)
library(glmnet)
library(caret)
library(pROC)
library(ggplot2)
library(dplyr)
library(reshape2)
library(gridExtra)
```

Load and clean Data

```{r, warning=FALSE}
#task mapping file
task <- read.delim("data/task.txt", header=TRUE)
colnames(task) = c("SampleID","y","SubjectID")

#OTU file (actual data)
otu <- read.delim("data/otutable.txt", 
                     header = TRUE, check.names = FALSE)
OTU = otu[,1]
count_data = t(otu[,-1])
colnames(count_data) = OTU

#all of the sample IDs should come up in the dataset
match_ids = sapply(task$SampleID, function(x) x %in% rownames(count_data))
match_ids

SampleID = rownames(count_data)

# Create true_y vector
true_y = rep(NA,nrow(count_data))
subject_id = rep(NA,nrow(count_data))
for(i in 1:nrow(count_data)){
  if(SampleID[i] %in% task$SampleID){
    true_y[i] = task$y[task$SampleID == SampleID[i]]
    subject_id[i] = task$SubjectID[task$SampleID == SampleID[i]]
  }
}
y = true_y
count_data_all = as.data.frame(cbind(SampleID, y, subject_id, count_data))
count_data_clean = count_data_all[complete.cases(count_data_all),]
View(count_data_clean)
write.csv(count_data_clean,"data/count_data_clean.csv")
```

## Methods

Approach 1: count data + lasso

```{r, warning=FALSE}
cv_count1 = cv.glmnet(x = count_data[complete.cases(count_data_all),], y = count_data_clean$y, family = "binomial")
count.glm.out = glmnet(x = count_data[complete.cases(count_data_all),], y = count_data_clean$y, family = "binomial",
                       lambda = cv_count1$lambda.min)
which(count.glm.out$beta != 0)
colnames(count_data[,which(count.glm.out$beta != 0)])
count.glm.out$beta[which(count.glm.out$beta != 0)]


# Split data into training and testing sets (70/30 split)
set.seed(123)
# Get unique subjects to ensure paired samples stay together
unique_subjects <- unique(count_data_clean$subject_id)
# Randomly select 70% of subjects for training
train_subjects <- sample(unique_subjects, floor(0.7 * length(unique_subjects)))
# Create indices for train/test split
train_idx <- which(count_data_clean$subject_id %in% train_subjects)
test_idx <- which(!count_data_clean$subject_id %in% train_subjects)

# Extract features and response
X <- count_data[complete.cases(count_data_all),]
# Make sure response is properly coded as factor with specific levels
y <- factor(ifelse(count_data_clean$y == "Tumor", "Tumor", "Healthy"), 
            levels = c("Healthy", "Tumor"))

# Create training and testing datasets
X_train <- X[train_idx,]
y_train <- y[train_idx]
X_test <- X[test_idx,]
y_test <- y[test_idx]

# Cross-validation to select optimal lambda
set.seed(123)
cv_count1 <- cv.glmnet(x = as.matrix(X_train), y = y_train, family = "binomial", 
                       type.measure = "auc", nfolds = 5)

# Plot cross-validation results
plot(cv_count1)
title("LASSO Cross-Validation with Raw Count Data")

# Get the optimal lambda value
lambda_min <- cv_count1$lambda.min
lambda_1se <- cv_count1$lambda.1se
cat("Optimal lambda (minimum):", lambda_min, "\n")
cat("Optimal lambda (1 standard error rule):", lambda_1se, "\n")

# Fit final model using optimal lambda
count_lasso_model <- glmnet(x = as.matrix(X_train), y = y_train, family = "binomial",
                          lambda = lambda_min)

# Identify features with non-zero coefficients
nonzero_idx <- which(count_lasso_model$beta != 0)
nonzero_features <- colnames(X)[nonzero_idx]
nonzero_coefs <- count_lasso_model$beta[nonzero_idx]

# Print number of selected features
cat("Number of features selected:", length(nonzero_idx), "out of", ncol(X), "\n")

# Create a data frame of important features and their coefficients
feature_importance <- data.frame(
  Feature = nonzero_features,
  Coefficient = as.vector(nonzero_coefs)
)

# Sort by absolute coefficient value
feature_importance <- feature_importance %>%
  arrange(desc(abs(Coefficient)))

# Display top 20 most important features
head(feature_importance, 20)

# Make predictions on the test set
test_probs <- predict(count_lasso_model, newx = as.matrix(X_test), type = "response", s = lambda_min)
# Convert probabilities to class predictions with the same levels as the reference
test_preds <- factor(ifelse(test_probs > 0.5, "Tumor", "Healthy"), levels = levels(y_test))

# Check that levels match before creating confusion matrix
print("Levels in test_preds:")
print(levels(test_preds))
print("Levels in y_test:")
print(levels(y_test))

# Evaluate model performance
conf_matrix <- confusionMatrix(test_preds, y_test)
print(conf_matrix)

# Calculate additional metrics
accuracy <- conf_matrix$overall["Accuracy"]
sensitivity <- conf_matrix$byClass["Sensitivity"]
specificity <- conf_matrix$byClass["Specificity"]
balanced_accuracy <- conf_matrix$byClass["Balanced Accuracy"]

# Print metrics
cat("\nModel Performance Metrics:\n")
cat("Accuracy:", round(accuracy, 4), "\n")
cat("Sensitivity:", round(sensitivity, 4), "\n")
cat("Specificity:", round(specificity, 4), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy, 4), "\n")

# Generate ROC curve
roc_obj <- roc(as.numeric(y_test == "Tumor"), as.vector(test_probs))
auc_value <- auc(roc_obj)
cat("AUC:", round(auc_value, 4), "\n")

# Plot ROC curve
plot(roc_obj, main = "ROC Curve for Raw Counts + LASSO Model", 
     col = "blue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "gray")
legend("bottomright", legend = paste("AUC =", round(auc_value, 4)), 
       col = "blue", lwd = 2)

# Save the model and results
raw_counts_results <- list(
  model = count_lasso_model,
  lambda_min = lambda_min,
  selected_features = feature_importance,
  performance = list(
    accuracy = accuracy,
    sensitivity = sensitivity,
    specificity = specificity,
    balanced_accuracy = balanced_accuracy,
    auc = auc_value,
    confusion_matrix = conf_matrix
  )
)
```

Approach 2: ALR transformation

```{r, warning=FALSE}
comp_data = count_data
for(i in 1:nrow(count_data)){
  comp_data[i,] = count_data[i,]/sum(count_data[i,])
}
rowSums(comp_data)
comp_data_all = as.data.frame(cbind(SampleID,y,subject_id,comp_data))
comp_data_clean = comp_data_all[complete.cases(comp_data_all),]
# write.csv(comp_data_clean,"comp_data_clean.csv")

alr_comp_data = alr(comp_data + 1e-30)
alr_comp_data_clean = alr_comp_data[complete.cases(comp_data_all),]

set.seed(123)
cv_glm1 = cv.glmnet(x = alr_comp_data_clean, y = comp_data_clean$y, family = "binomial")
glm.out = glmnet(x = alr_comp_data_clean, y = comp_data_clean$y, family = "binomial", lambda = cv_glm1$lambda.min)
which(glm.out$beta != 0)


# Split data into training and testing sets (using same split as Approach 1)
set.seed(123)
# Get unique subjects to ensure paired samples stay together
unique_subjects <- unique(count_data_clean$subject_id)
# Randomly select 70% of subjects for training
train_subjects <- sample(unique_subjects, floor(0.7 * length(unique_subjects)))
# Create indices for train/test split
train_idx <- which(count_data_clean$subject_id %in% train_subjects)
test_idx <- which(!count_data_clean$subject_id %in% train_subjects)

# Generate compositional data (proportions)
comp_data = count_data[complete.cases(count_data_all),]
for(i in 1:nrow(comp_data)){
  comp_data[i,] = comp_data[i,]/sum(comp_data[i,])
}

# Verify all rows sum to 1
all(abs(rowSums(comp_data) - 1) < 1e-10)

# Apply ALR transformation (adding small value to avoid log(0))
alr_comp_data = alr(comp_data + 1e-30)

# Extract response variable
y <- as.factor(count_data_clean$y)

# Create training and testing datasets
X_alr_train <- alr_comp_data[train_idx,]
y_train <- y[train_idx]
X_alr_test <- alr_comp_data[test_idx,]
y_test <- y[test_idx]

# Cross-validation to select optimal lambda
set.seed(123)
cv_alr <- cv.glmnet(x = as.matrix(X_alr_train), y = y_train, family = "binomial",
                   type.measure = "auc", nfolds = 5)

# Plot cross-validation results
plot(cv_alr)
title("LASSO Cross-Validation with ALR Transformed Data")

# Get the optimal lambda value
lambda_min <- cv_alr$lambda.min
lambda_1se <- cv_alr$lambda.1se
cat("Optimal lambda (minimum):", lambda_min, "\n")
cat("Optimal lambda (1 standard error rule):", lambda_1se, "\n")

# Fit final model using optimal lambda
alr_lasso_model <- glmnet(x = as.matrix(X_alr_train), y = y_train, family = "binomial",
                        lambda = lambda_min)

# Identify features with non-zero coefficients
nonzero_idx <- which(alr_lasso_model$beta != 0)
nonzero_features <- colnames(alr_comp_data)[nonzero_idx]
nonzero_coefs <- alr_lasso_model$beta[nonzero_idx]

# Print number of selected features
cat("Number of features selected:", length(nonzero_idx), "out of", ncol(alr_comp_data), "\n")

# Create a data frame of important features and their coefficients
feature_importance <- data.frame(
  Feature = nonzero_features,
  Coefficient = as.vector(nonzero_coefs)
)

# Sort by absolute coefficient value
feature_importance <- feature_importance %>%
  arrange(desc(abs(Coefficient)))

# Display top 20 most important features
head(feature_importance, 20)

# Make predictions on the test set
test_probs <- predict(alr_lasso_model, newx = as.matrix(X_alr_test), type = "response", s = lambda_min)
test_preds <- ifelse(test_probs > 0.5, 1, 0)

# Evaluate model performance
conf_matrix <- confusionMatrix(as.factor(test_preds), y_test)
print(conf_matrix)

# Calculate additional metrics
accuracy <- conf_matrix$overall["Accuracy"]
sensitivity <- conf_matrix$byClass["Sensitivity"]
specificity <- conf_matrix$byClass["Specificity"]
balanced_accuracy <- conf_matrix$byClass["Balanced Accuracy"]

# Print metrics
cat("\nModel Performance Metrics:\n")
cat("Accuracy:", round(accuracy, 4), "\n")
cat("Sensitivity:", round(sensitivity, 4), "\n")
cat("Specificity:", round(specificity, 4), "\n")
cat("Balanced Accuracy:", round(balanced_accuracy, 4), "\n")

# Generate ROC curve
roc_obj <- roc(y_test, as.vector(test_probs))
auc_value <- auc(roc_obj)
cat("AUC:", round(auc_value, 4), "\n")

# Plot ROC curve
plot(roc_obj, main = "ROC Curve for ALR + LASSO Model", 
     col = "red", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "gray")
legend("bottomright", legend = paste("AUC =", round(auc_value, 4)), 
       col = "red", lwd = 2)

# Feature importance visualization
if (length(nonzero_idx) > 0) {
  # Select top 20 features for visualization
  top_features <- head(feature_importance, 20)
  
  # Create bar plot of feature importance
  ggplot(top_features, aes(x = reorder(Feature, abs(Coefficient)), y = Coefficient, 
                          fill = Coefficient > 0)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    scale_fill_manual(values = c("red", "blue"), 
                     labels = c("Negative Association", "Positive Association"),
                     name = "Direction") +
    labs(title = "Top 20 Log-Ratio Features Selected by ALR + LASSO",
         x = "Log-Ratio Features",
         y = "Coefficient Value") +
    theme_minimal()
}

# Interpret ALR features
# In ALR transformation, each feature represents log(x_i/x_ref)
# For classification, positive coefficients mean higher ratios are associated with tumors
# Negative coefficients mean lower ratios are associated with tumors

# For better interpretation, we can find the denominator (reference taxon) used in ALR
ref_taxon <- attr(alr_comp_data, "orig.names")[attr(alr_comp_data, "dim")[2]+1]
cat("Reference taxon for ALR transformation:", ref_taxon, "\n")

# Save the model and results
alr_results <- list(
  model = alr_lasso_model,
  lambda_min = lambda_min,
  selected_features = feature_importance,
  reference_taxon = ref_taxon,
  performance = list(
    accuracy = accuracy,
    sensitivity = sensitivity,
    specificity = specificity,
    balanced_accuracy = balanced_accuracy,
    auc = auc_value,
    confusion_matrix = conf_matrix
  )
)
```

Approach 3: Zero-Sum Regression for Compositional Covariates (RobZS and FLORAL)

```{r, include = FALSE}

```

```{r}

```

Comparison of Classification Methods:

library(reshape2)

library(gridExtra)

```{r, warning=FALSE}


# Assuming raw_counts_results and alr_results are available from previous analyses

# Compare performance metrics
performance_comparison <- data.frame(
  Method = c("Raw Counts + LASSO", "ALR Transformation + LASSO"),
  Accuracy = c(raw_counts_results$performance$accuracy, alr_results$performance$accuracy),
  Sensitivity = c(raw_counts_results$performance$sensitivity, alr_results$performance$sensitivity),
  Specificity = c(raw_counts_results$performance$specificity, alr_results$performance$specificity),
  Balanced_Accuracy = c(raw_counts_results$performance$balanced_accuracy, alr_results$performance$balanced_accuracy),
  AUC = c(raw_counts_results$performance$auc, alr_results$performance$auc)
)

# Display performance comparison
print(performance_comparison)

# Visualize performance metrics comparison
performance_long <- melt(performance_comparison, id.vars = "Method")
ggplot(performance_long, aes(x = variable, y = value, fill = Method)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.7) +
  scale_fill_manual(values = c("steelblue", "firebrick")) +
  labs(title = "Performance Comparison: Raw Counts vs. ALR Transformation",
       x = "Metric", y = "Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_text(aes(label = sprintf("%.3f", value)), 
            position = position_dodge(width = 0.7), 
            vjust = -0.3, size = 3)

# Compare number of selected features
feature_count_comparison <- data.frame(
  Method = c("Raw Counts + LASSO", "ALR Transformation + LASSO"),
  Number_of_Features = c(nrow(raw_counts_results$selected_features), 
                         nrow(alr_results$selected_features))
)

# Display feature count comparison
print(feature_count_comparison)

# Visualize feature count comparison
ggplot(feature_count_comparison, aes(x = Method, y = Number_of_Features, fill = Method)) +
  geom_bar(stat = "identity", width = 0.5) +
  scale_fill_manual(values = c("steelblue", "firebrick")) +
  labs(title = "Number of Selected Features by Method",
       x = "", y = "Number of Features") +
  theme_minimal() +
  geom_text(aes(label = Number_of_Features), vjust = -0.3)

# Compare ROC curves
# Create a plot with both ROC curves
roc_raw <- roc(y_test, as.vector(predict(raw_counts_results$model, 
                                         newx = as.matrix(X_test), 
                                         type = "response", 
                                         s = raw_counts_results$lambda_min)))

roc_alr <- roc(y_test, as.vector(predict(alr_results$model, 
                                        newx = as.matrix(X_alr_test), 
                                        type = "response", 
                                        s = alr_results$lambda_min)))

plot(roc_raw, col = "blue", lwd = 2, 
     main = "ROC Curve Comparison")
lines(roc_alr, col = "red", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "gray")
legend("bottomright", 
       legend = c(
         paste("Raw Counts (AUC =", round(raw_counts_results$performance$auc, 3), ")"),
         paste("ALR (AUC =", round(alr_results$performance$auc, 3), ")")
       ),
       col = c("blue", "red"), 
       lwd = 2)

# Look at overlap in selected features
if (length(raw_counts_results$selected_features$Feature) > 0 && 
    length(alr_results$selected_features$Feature) > 0) {
  
  # For ALR, we need to extract the numerator of each log-ratio
  alr_numerators <- sapply(strsplit(as.character(alr_results$selected_features$Feature), "/"), 
                          function(x) x[1])
  
  # Find common features
  common_features <- intersect(raw_counts_results$selected_features$Feature,
                              alr_numerators)
  
  cat("Number of common features selected by both methods:", 
      length(common_features), "\n")
  
  if (length(common_features) > 0) {
    cat("Common features:\n")
    print(common_features)
    
    # Create Venn diagram-like display of feature overlap
    library(VennDiagram)
    venn.plot <- venn.diagram(
      x = list(
        "Raw Counts" = raw_counts_results$selected_features$Feature,
        "ALR Transform" = alr_numerators
      ),
      filename = NULL,
      fill = c("steelblue", "firebrick"),
      alpha = 0.5,
      main = "Overlap in Selected Features"
    )
    grid.draw(venn.plot)
  }
}

# Provide a summary of findings
cat("\n==== Summary of Findings ====\n")

cat("\n1. Classification Performance:\n")
if (raw_counts_results$performance$auc > alr_results$performance$auc) {
  cat("   - The Raw Counts + LASSO approach achieved better overall performance with higher AUC.\n")
} else if (raw_counts_results$performance$auc < alr_results$performance$auc) {
  cat("   - The ALR Transformation + LASSO approach achieved better overall performance with higher AUC.\n")
} else {
  cat("   - Both approaches achieved similar overall performance in terms of AUC.\n")
}

cat("\n2. Feature Selection:\n")
cat("   - Raw Counts + LASSO selected", nrow(raw_counts_results$selected_features), "features.\n")
cat("   - ALR Transformation + LASSO selected", nrow(alr_results$selected_features), "features.\n")

cat("\n3. Biological Insights:\n")
cat("   - Both methods identified taxonomic patterns associated with colorectal cancer.\n")
cat("   - The ALR method may provide more statistically valid insights due to accounting for the compositional nature of microbiome data.\n")
cat("   - The Raw Counts method may be more directly interpretable in terms of abundance differences between tumor and normal samples.\n")

# Recommendations for practice
cat("\n4. Recommendations:\n")
cat("   - Consider using both methods complementarily for microbiome analysis.\n")
cat("   - Raw counts may be more intuitive for initial exploratory analysis.\n")
cat("   - ALR transformation provides a statistically principled approach for compositional data.\n")
cat("   - Features consistently identified by both methods may be the most reliable biomarkers.\n")
```
